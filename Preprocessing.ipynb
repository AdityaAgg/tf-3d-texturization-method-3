{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lib.binvox_rw.binvox_rw as binvox_rw\n",
    "import pickle\n",
    "import PIL\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_voxels(voxels, size):\n",
    "    def resize_voxels_op(voxels, size):\n",
    "        resized_voxels = voxels\n",
    "        curr_shape = tf.shape(voxels)\n",
    "        batch_size = curr_shape[0]\n",
    "        curr_size = curr_shape[1]\n",
    "        denom = curr_size // size\n",
    "\n",
    "        for view, shape in [[[0, 3, 2, 1], [batch_size, curr_size, curr_size, size, denom]],\n",
    "                            [[0, 3, 1, 2], [batch_size, size, curr_size, size, denom]], \n",
    "                            [[0, 1, 3, 2], [batch_size, size, size, size, denom]]]:\n",
    "            resized_voxels = tf.reduce_any(tf.reshape(resized_voxels, shape), axis=4)\n",
    "            resized_voxels = tf.transpose(resized_voxels, view)\n",
    "\n",
    "        return resized_voxels\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    voxels_placeholder = tf.placeholder(bool, shape=voxels.shape)\n",
    "    size_placeholder = tf.placeholder(tf.int32)\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=config)\n",
    "    print config\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    resized_voxels = sess.run(resize_voxels_op(voxels_placeholder, size_placeholder), \n",
    "                              feed_dict={voxels_placeholder: voxels, size_placeholder: size})\n",
    "\n",
    "    return resized_voxels\n",
    "\n",
    "def resize_images(image, size):\n",
    "    return image.resize((size, size), PIL.Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_voxels(model_path, model_ids, size=None):\n",
    "    voxels = []\n",
    "    \n",
    "    for model_id in model_ids:\n",
    "        binvox_id = \"{}{}.binvox\".format(model_path, model_id)\n",
    "        \n",
    "        with open(binvox_id, 'rb') as f:\n",
    "            model = binvox_rw.read_as_3d_array(f)\n",
    "            voxels.append(model.data)\n",
    "            \n",
    "    voxels = np.asarray(voxels, dtype=bool)\n",
    "    \n",
    "                 \n",
    "                    \n",
    "    if size is not None:\n",
    "        voxels = resize_voxels(voxels, size)\n",
    "        voxels = center_voxels(voxels)\n",
    "    \n",
    "    \n",
    "    return voxels \n",
    "\n",
    "def center_voxels(voxels):\n",
    "    \n",
    "    \n",
    "    sample_num, x_dim, y_dim, z_dim = voxels.shape\n",
    "    min_x = x // 2\n",
    "    min_y = y // 2\n",
    "    min_z = z // 2\n",
    "    max_x = x // 2\n",
    "    max_y = y // 2\n",
    "    max_z = z // 2\n",
    "    \n",
    "    step = 0\n",
    "    for sample_i in voxels:\n",
    "        \n",
    "        #find min and max\n",
    "        for xi in xrange(x):\n",
    "            for yi in xrange(y): \n",
    "                for zi in xrange(z): \n",
    "                    if sample_i[xi, yi, zi] is True: \n",
    "                        if min_x > xi: \n",
    "                            min_x = xi\n",
    "                        elif max_x < xi: \n",
    "                            max_x = xi\n",
    "\n",
    "                        if min_y > yi: \n",
    "                            min_y = yi\n",
    "                        elif max_y < yi: \n",
    "                            max_y = yi\n",
    "\n",
    "                        if min_z > zi: \n",
    "                            min_z = zi\n",
    "                        elif max_z < zi: \n",
    "                            max_z = zi\n",
    "\n",
    "        x_mid = (max_x + min_x)//2 \n",
    "        y_mid = (max_y + min_y)//2\n",
    "        z_mid = (max_z + min_z)//2\n",
    "\n",
    "        x_mid/=2\n",
    "        y_mid/=2\n",
    "        z_mid/=2\n",
    "        \n",
    "        \n",
    "        # translate\n",
    "        translate_z = z//2 - z_mid \n",
    "        translate_y = y//2 - y_mid\n",
    "        translate_x = x//2 - x_mid\n",
    "        zeros_like_sample_i = np.zeros_like(sample_i)\n",
    "        zeros_like_sample_i[:, :, np.arange(0, z-translate_z) +translate_z] = sample_i[:, :, np.arange(0, z-translate_z)]\n",
    "        zeros_like_sample_i[:, :, np.arange(0, y-translate_y) +translate_y] = sample_i[:, :, np.arange(0, y-translate_y)]\n",
    "        zeros_like_sample_i[:, :, np.arange(0, x-translate_x) +translate_x] = sample_i[:, :, np.arange(0, x-translate_x)]\n",
    "        voxels[i] = zeros_like_sample_i\n",
    "        step+=1\n",
    "        \n",
    "    return voxels \n",
    "\n",
    "def load_ids(filename):\n",
    "    df = pd.read_csv(\"data/metadata.csv\")\n",
    "    saved_column = df.fullId.values.astype(str)\n",
    "    front = df.front.values(str)\n",
    "    up = df.up.values(str)\n",
    "    model_ids = np.char.lstrip(saved_column, \"wss.\")\n",
    "    \n",
    "    return model_ids, front, up\n",
    "\n",
    "def load_images(image_path, model_ids, count,  size=None):\n",
    "    images = [[] for _ in range(count)]\n",
    "    step =0\n",
    "    for model_id in model_ids:\n",
    "        step+=1\n",
    "        for i in range(count):\n",
    "            image_file =  \"{}{}/{}-{}.png\".format(image_path, model_id, model_id, i)\n",
    "            with PIL.Image.open(image_file) as image:\n",
    "                resized = resize_images(image, size)\n",
    "                images[i].append(np.asarray(resized))\n",
    "            #if step%200 == 0: \n",
    "             #   print \"Step \", step\n",
    "           \n",
    "    \n",
    "    return np.asarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the 6 faces from voxels\n",
    "def voxel_to_faces(voxel):\n",
    "    p0 = voxel.transpose((1,0,2)) # front y z x\n",
    "    p1 = voxel.transpose((1,2,0)) # back y x z\n",
    "    p2 = voxel.transpose((0,2,1)) # left z x y\n",
    "    p3 = voxel                    # right z y x\n",
    "    p4 = voxel.transpose((2,0,1)) # top x z y\n",
    "    p5 = voxel.transpose((2,1,0)) # bottom x y z\n",
    "    \n",
    "    p = np.stack((p0, p1, p2, p3, p4, p5))\n",
    "    faces = np.where(p.any(1), p.argmax(1), -1)\n",
    "\n",
    "    return faces\n",
    "\n",
    "# pass images in order front > back > left > right > top > bottom\n",
    "# changes image orientation to match faces\n",
    "def fix_image_orientation(images):\n",
    "    fixed_images = np.stack([\n",
    "        np.flip(images[0].transpose((1,0,2)), axis=1),\n",
    "        np.flip(np.flip(images[1], axis=1), axis=0),\n",
    "        np.flip(np.flip(images[2], axis=1), axis=0),\n",
    "        np.flip(images[3].transpose((1,0,2)), axis=1),\n",
    "        images[4].transpose((1,0,2)),\n",
    "        np.flip(images[5].transpose((1,0,2)), axis=1)\n",
    "    ])\n",
    "             \n",
    "    return fixed_images\n",
    "\n",
    "# pass images in order front > back > left > right > top > bottom\n",
    "# front refers to side facing us when plotted and not the front based on object\n",
    "def color_voxel(voxel, images):\n",
    "    colored_voxel = np.ones(voxel.shape + (3,))\n",
    "    views = [\n",
    "        (1,0,2,3),\n",
    "        (0,2,1,3),\n",
    "        (2,1,0,3),\n",
    "        (0,2,1,3),\n",
    "        (2,0,1,3),\n",
    "        (0,2,1,3)\n",
    "    ]\n",
    "    \n",
    "    faces = voxel_to_faces(voxel)\n",
    "    orientated_images = fix_image_orientation(images)\n",
    "    \n",
    "    colors = np.zeros(faces.shape + (3,))\n",
    "    colors[np.where(faces > -1)] = [0.5, 0.5, 0.5]\n",
    "    \n",
    "    for c, im in zip(colors, orientated_images):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 10))\n",
    "        fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "        \n",
    "        ax = axes.flat[0]\n",
    "        ax.imshow(c)\n",
    "        ax.set_xlabel(\"Shape\")\n",
    "\n",
    "        # Plot the mixed-image.\n",
    "        ax = axes.flat[1]\n",
    "        ax.imshow(im[:,:,:3] / 255.0)\n",
    "        ax.set_xlabel(\"Image\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    for i, view in enumerate(views):\n",
    "        face_mask = np.where(faces[i] > -1)\n",
    "        colored_voxel = colored_voxel.transpose(view)\n",
    "        colored_voxel[(faces[i][face_mask],) + face_mask] = orientated_images[i][face_mask][:,:3] / 255.\n",
    "        \n",
    "    print(np.sum(((voxel[(faces[3][np.where(faces[3] > -1)],) + np.where(faces[3] > -1)]) == False) * 1))\n",
    "    \n",
    "    return colored_voxel.transpose((2,1,0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(voxels, views, path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "    with open(\"{}voxels.pickle\".format(path), 'wb') as f:\n",
    "        pickle.dump(voxels, f)\n",
    "        \n",
    "    for i, view in enumerate(views):\n",
    "        with open(\"{}view-{}.pickle\".format(path, i), 'wb') as f:\n",
    "            pickle.dump(view, f)\n",
    "        \n",
    "def read_data(path, num_views):\n",
    "    views = []\n",
    "    \n",
    "    with open(\"{}voxels.pickle\".format(path), 'rb') as f:\n",
    "        voxels = pickle.load(f)\n",
    "    \n",
    "    for i in range(num_views):\n",
    "        with open(\"{}view-{}.pickle\".format(path, i), 'rb') as f:\n",
    "            views.append(pickle.load(f))\n",
    "    \n",
    "    return voxels, views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = \"data/models-binvox/\"\n",
    "image_path = \"data/screenshots/\"\n",
    "metadata_file = \"data/metadata.csv\"\n",
    "\n",
    "\n",
    "model_ids, front, up = load_ids(metadata_file)\n",
    "# Use above to load all model ids. Below is hardcoded due to missing files.\n",
    "# model_ids = ['2fa5e7c9e378b2589cd62032ff4f127f', '3b30f1acc693d6a2308715fe727b3f72']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6802, 32, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "t = color_voxel(voxels[1], [images[4,1], images[5,1], images[1,1], images[0,1], images[3,1], images[2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_options {\n",
      "  allow_growth: true\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 24] Too many open files: 'data/screenshots/1004f30be305f33d28a1548e344f0e2e/1004f30be305f33d28a1548e344f0e2e-0.png'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e2378a13951e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmetadata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/metadata.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvoxels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_voxels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-c99ac85c2fa7>\u001b[0m in \u001b[0;36mload_images\u001b[0;34m(image_path, model_ids, count, size)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mimage_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}{}/{}-{}.png\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mresized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adityaaggarwal/miniconda3/envs/py27pix2vox/lib/python2.7/site-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2476\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2477\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2478\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 24] Too many open files: 'data/screenshots/1004f30be305f33d28a1548e344f0e2e/1004f30be305f33d28a1548e344f0e2e-0.png'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "model_path = \"data/models-binvox/\"\n",
    "image_path = \"data/screenshots/\"\n",
    "metadata_file = \"data/metadata.csv\"\n",
    "voxels = load_voxels(model_path, model_ids, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  200\n",
      "Step  200\n",
      "Step  200\n",
      "Step  200\n",
      "Step  200\n",
      "Step  200\n",
      "Step  400\n",
      "Step  400\n",
      "Step  400\n",
      "Step  400\n",
      "Step  400\n",
      "Step  400\n",
      "Step  600\n",
      "Step  600\n",
      "Step  600\n",
      "Step  600\n",
      "Step  600\n",
      "Step  600\n",
      "Step  800\n",
      "Step  800\n",
      "Step  800\n",
      "Step  800\n",
      "Step  800\n",
      "Step  800\n",
      "Step  1000\n",
      "Step  1000\n",
      "Step  1000\n",
      "Step  1000\n",
      "Step  1000\n",
      "Step  1000\n",
      "Step  1200\n",
      "Step  1200\n",
      "Step  1200\n",
      "Step  1200\n",
      "Step  1200\n",
      "Step  1200\n",
      "Step  1400\n",
      "Step  1400\n",
      "Step  1400\n",
      "Step  1400\n",
      "Step  1400\n",
      "Step  1400\n",
      "Step  1600\n",
      "Step  1600\n",
      "Step  1600\n",
      "Step  1600\n",
      "Step  1600\n",
      "Step  1600\n",
      "Step  1800\n",
      "Step  1800\n",
      "Step  1800\n",
      "Step  1800\n",
      "Step  1800\n",
      "Step  1800\n",
      "Step  2000\n",
      "Step  2000\n",
      "Step  2000\n",
      "Step  2000\n",
      "Step  2000\n",
      "Step  2000\n",
      "Step  2200\n",
      "Step  2200\n",
      "Step  2200\n",
      "Step  2200\n",
      "Step  2200\n",
      "Step  2200\n",
      "Step  2400\n",
      "Step  2400\n",
      "Step  2400\n",
      "Step  2400\n",
      "Step  2400\n",
      "Step  2400\n",
      "Step  2600\n",
      "Step  2600\n",
      "Step  2600\n",
      "Step  2600\n",
      "Step  2600\n",
      "Step  2600\n",
      "Step  2800\n",
      "Step  2800\n",
      "Step  2800\n",
      "Step  2800\n",
      "Step  2800\n",
      "Step  2800\n",
      "Step  3000\n",
      "Step  3000\n",
      "Step  3000\n",
      "Step  3000\n",
      "Step  3000\n",
      "Step  3000\n",
      "Step  3200\n",
      "Step  3200\n",
      "Step  3200\n",
      "Step  3200\n",
      "Step  3200\n",
      "Step  3200\n",
      "Step  3400\n",
      "Step  3400\n",
      "Step  3400\n",
      "Step  3400\n",
      "Step  3400\n",
      "Step  3400\n",
      "Step  3600\n",
      "Step  3600\n",
      "Step  3600\n",
      "Step  3600\n",
      "Step  3600\n",
      "Step  3600\n",
      "Step  3800\n",
      "Step  3800\n",
      "Step  3800\n",
      "Step  3800\n",
      "Step  3800\n",
      "Step  3800\n",
      "Step  4000\n",
      "Step  4000\n",
      "Step  4000\n",
      "Step  4000\n",
      "Step  4000\n",
      "Step  4000\n",
      "Step  4200\n",
      "Step  4200\n",
      "Step  4200\n",
      "Step  4200\n",
      "Step  4200\n",
      "Step  4200\n",
      "Step  4400\n",
      "Step  4400\n",
      "Step  4400\n",
      "Step  4400\n",
      "Step  4400\n",
      "Step  4400\n",
      "Step  4600\n",
      "Step  4600\n",
      "Step  4600\n",
      "Step  4600\n",
      "Step  4600\n",
      "Step  4600\n",
      "Step  4800\n",
      "Step  4800\n",
      "Step  4800\n",
      "Step  4800\n",
      "Step  4800\n",
      "Step  4800\n",
      "Step  5000\n",
      "Step  5000\n",
      "Step  5000\n",
      "Step  5000\n",
      "Step  5000\n",
      "Step  5000\n",
      "Step  5200\n",
      "Step  5200\n",
      "Step  5200\n",
      "Step  5200\n",
      "Step  5200\n",
      "Step  5200\n",
      "Step  5400\n",
      "Step  5400\n",
      "Step  5400\n",
      "Step  5400\n",
      "Step  5400\n",
      "Step  5400\n",
      "Step  5600\n",
      "Step  5600\n",
      "Step  5600\n",
      "Step  5600\n",
      "Step  5600\n",
      "Step  5600\n",
      "Step  5800\n",
      "Step  5800\n",
      "Step  5800\n",
      "Step  5800\n",
      "Step  5800\n",
      "Step  5800\n",
      "Step  6000\n",
      "Step  6000\n",
      "Step  6000\n",
      "Step  6000\n",
      "Step  6000\n",
      "Step  6000\n",
      "Step  6200\n",
      "Step  6200\n",
      "Step  6200\n",
      "Step  6200\n",
      "Step  6200\n",
      "Step  6200\n",
      "Step  6400\n",
      "Step  6400\n",
      "Step  6400\n",
      "Step  6400\n",
      "Step  6400\n",
      "Step  6400\n",
      "Step  6600\n",
      "Step  6600\n",
      "Step  6600\n",
      "Step  6600\n",
      "Step  6600\n",
      "Step  6600\n",
      "Step  6800\n",
      "Step  6800\n",
      "Step  6800\n",
      "Step  6800\n",
      "Step  6800\n",
      "Step  6800\n"
     ]
    }
   ],
   "source": [
    "images = load_images(image_path, model_ids, 6, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path= \"data/preprocessed/\"\n",
    "\n",
    "\n",
    "save_data(voxels, images, save_path)\n",
    "voxels, images = read_data(save_path, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6802\n"
     ]
    }
   ],
   "source": [
    "print (model_ids.size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
